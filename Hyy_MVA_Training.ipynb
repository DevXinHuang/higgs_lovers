{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07e835a",
   "metadata": {},
   "source": [
    "# H→γγ MVA Training Notebook\n",
    "\n",
    "This notebook trains a Neural Network to distinguish H→γγ signal events from background events using Monte Carlo (MC) simulations.\n",
    "The trained model will then be used in the main H→γγ analysis notebook (`HyyAnalysisNew.ipynb`) to improve event selection.\n",
    "\n",
    "Steps:\n",
    "1. Configure paths to MC signal and background samples.\n",
    "2. Define features for the MVA.\n",
    "3. Load and preprocess MC data.\n",
    "4. Train a Neural Network classifier.\n",
    "5. Evaluate the model's performance.\n",
    "6. Save the trained model and feature scaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a57251",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f08f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import joblib # For saving/loading model and scaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import vector # For 4-vector calculations\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-pastel')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Ensure the vector library is aware of 'mass' attribute if not already default\n",
    "# vector.register_awkward() # May not be needed depending on vector version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49465343",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f73556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set.\n",
      "MC Signal pattern: /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712.*.pool.root.1\n",
      "MC Background pattern: /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.37110937._000011.pool.root.1\n",
      "MVA Model will be saved to: /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/hyy_mva_model.pkl\n",
      "MVA Scaler will be saved to: /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/hyy_mva_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# --- User Configuration ---\n",
    "\n",
    "# Base directory (assuming this notebook is in 'higgs_lovers')\n",
    "notebook_base_dir = os.getcwd()\n",
    "data_dir = os.path.join(notebook_base_dir, \"data\") # Standard data directory\n",
    "\n",
    "# !! PLEASE UPDATE THESE PATHS TO YOUR H→γγ MC FILES !!\n",
    "# Example placeholder paths:\n",
    "mc_signal_hyy_file_pattern = os.path.join(data_dir, \"DAOD_PHYSLITE.38191712.*.pool.root.1\") # e.g., ggH production\n",
    "mc_background_hyy_file_pattern = os.path.join(data_dir, \"DAOD_PHYSLITE.37110937._000011.pool.root.1\") # e.g., direct diphoton production\n",
    "# You might have multiple background sources to combine.\n",
    "\n",
    "# Tree name in your MC ROOT files\n",
    "tree_name = \"analysis\" # Same as in HyyAnalysisNew.ipynb\n",
    "\n",
    "# Output paths for the trained model and scaler\n",
    "output_model_dir = notebook_base_dir\n",
    "mva_model_path = os.path.join(output_model_dir, \"hyy_mva_model.pkl\")\n",
    "mva_scaler_path = os.path.join(output_model_dir, \"hyy_mva_scaler.pkl\")\n",
    "\n",
    "# Branches to read from MC files (ensure these cover all features needed for MVA and pre-selection)\n",
    "# These should be similar to HyyAnalysisNew.ipynb, plus any additional ones for MVA features.\n",
    "branches_to_read_mc = [\n",
    "    \"photon_pt\", \"photon_eta\", \"photon_phi\", \"photon_e\",\n",
    "    \"photon_isTightID\", \"photon_ptcone20\",\n",
    "    \"mcEventWeight\" # Assuming there's an MC event weight branch\n",
    "    # Add other branches if needed for more sophisticated features\n",
    "]\n",
    "\n",
    "# Define MVA features (list of column names that will be created in the DataFrame)\n",
    "# IMPORTANT: Do NOT use m_yy (diphoton invariant mass) directly if you plan to fit the m_yy spectrum later.\n",
    "mva_features_list = [\n",
    "    'ph1_pt_norm', 'ph2_pt_norm', # Normalized pT by m_yy (can be okay if m_yy itself isn't a feature)\n",
    "    'ph1_eta', 'ph2_eta',\n",
    "    'delta_eta_yy', 'delta_phi_yy', 'delta_R_yy',\n",
    "    'ph1_ptcone20_norm', 'ph2_ptcone20_norm', # Isolation normalized by photon pT\n",
    "    # 'ph1_isTightID', 'ph2_isTightID', # Can be used if converted to numerical (0 or 1)\n",
    "    'cos_theta_cs_yy' # Cosine of Collins-Soper angle (sensitive to spin)\n",
    "    # Add more features as desired\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Configuration set.\")\n",
    "print(f\"MC Signal pattern: {mc_signal_hyy_file_pattern}\")\n",
    "print(f\"MC Background pattern: {mc_background_hyy_file_pattern}\")\n",
    "print(f\"MVA Model will be saved to: {mva_model_path}\")\n",
    "print(f\"MVA Scaler will be saved to: {mva_scaler_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118565b",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing Function for MC\n",
    "\n",
    "This function will load data from ROOT files, apply initial selections (similar to `HyyAnalysisNew.ipynb` but adapted for MC and MVA features), and calculate MVA input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4098cec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC loading and preprocessing function defined.\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_mc_for_mva(file_paths_pattern, is_signal_flag, tree_name_param, branches_param):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses MC data for MVA training.\n",
    "    - Reads specified branches.\n",
    "    - Applies basic quality cuts (from HyyAnalysisNew.ipynb).\n",
    "    - Calculates MVA features.\n",
    "    - Returns a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    all_files = glob.glob(file_paths_pattern)\n",
    "    if not all_files:\n",
    "        print(f\"No files found for pattern: {file_paths_pattern}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Processing {len(all_files)} files for pattern: {file_paths_pattern}...\")\n",
    "    \n",
    "    df_list = []\n",
    "\n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            with uproot.open(file_path + \":\" + tree_name_param) as tree:\n",
    "                # Limit entries for faster testing if needed: entry_stop=\"10000\"\n",
    "                events = tree.arrays(branches_param, library=\"ak\") \n",
    "        except Exception as e:\n",
    "            print(f\"Error opening or reading file {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if len(events) == 0:\n",
    "            print(f\"No events in {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # --- Apply Pre-selections (adapted from HyyAnalysisNew.ipynb) ---\n",
    "        # Ensure at least 2 photons\n",
    "        n_photons = ak.num(events.photon_pt)\n",
    "        events = events[n_photons >= 2]\n",
    "        if len(events) == 0: continue\n",
    "\n",
    "        # 1. Photon Reconstruction Quality (isTightID)\n",
    "        # Consider only events where the two leading photons pass isTightID\n",
    "        # (HyyAnalysisNew applies this after selecting 2 photons)\n",
    "        # For MVA, we might want to keep events even if one is not tight and let MVA learn\n",
    "        # For simplicity here, let's require both leading to be tight initially.\n",
    "        # This can be relaxed, and isTightID can become an MVA feature.\n",
    "        tight_id_cut = (events.photon_isTightID[:,0] == True) & (events.photon_isTightID[:,1] == True)\n",
    "        events = events[tight_id_cut]\n",
    "        if len(events) == 0: continue\n",
    "            \n",
    "        # 2. Photon pT cuts (leading > 50 GeV, sub-leading > 30 GeV)\n",
    "        # These are relatively high cuts from the paper. For MVA, sometimes looser cuts are used.\n",
    "        # Let's keep them for now as a baseline.\n",
    "        pt_cut = (events.photon_pt[:,0] > 50) & (events.photon_pt[:,1] > 30) # Assuming GeV\n",
    "        events = events[pt_cut]\n",
    "        if len(events) == 0: continue\n",
    "\n",
    "        # 3. Calorimeter Isolation (ptcone20 / pt < 0.055)\n",
    "        iso_cut = ((events.photon_ptcone20[:,0] / events.photon_pt[:,0]) < 0.055) & \\\n",
    "                    ((events.photon_ptcone20[:,1] / events.photon_pt[:,1]) < 0.055)\n",
    "        events = events[iso_cut]\n",
    "        if len(events) == 0: continue\n",
    "\n",
    "        # 4. Eta Transition Region (|eta| not between 1.37 and 1.52)\n",
    "        eta_cut_ph0 = ~((np.abs(events.photon_eta[:,0]) > 1.37) & (np.abs(events.photon_eta[:,0]) < 1.52))\n",
    "        eta_cut_ph1 = ~((np.abs(events.photon_eta[:,1]) > 1.37) & (np.abs(events.photon_eta[:,1]) < 1.52))\n",
    "        events = events[eta_cut_ph0 & eta_cut_ph1]\n",
    "        if len(events) == 0: continue\n",
    "\n",
    "        # --- Calculate Diphoton Invariant Mass (m_yy) ---\n",
    "        # Needed for some normalized features, but NOT as a direct MVA input.\n",
    "        p4_photons = vector.zip({\n",
    "            \"pt\": events.photon_pt, \"eta\": events.photon_eta,\n",
    "            \"phi\": events.photon_phi, \"e\": events.photon_e\n",
    "        })\n",
    "        diphoton = p4_photons[:,0] + p4_photons[:,1]\n",
    "        m_yy = diphoton.mass\n",
    "        \n",
    "        # 5. Mass-based Isolation (pt_gamma / m_yy > 0.35)\n",
    "        # This cut from HyyAnalysisNew uses m_yy.\n",
    "        mass_iso_cut = (events.photon_pt[:,0] / m_yy > 0.35) & (events.photon_pt[:,1] / m_yy > 0.35)\n",
    "        events = events[mass_iso_cut]\n",
    "        if len(events) == 0: continue\n",
    "        \n",
    "        # Re-calculate m_yy for the events that passed all cuts\n",
    "        p4_photons_final = vector.zip({\n",
    "            \"pt\": events.photon_pt, \"eta\": events.photon_eta,\n",
    "            \"phi\": events.photon_phi, \"e\": events.photon_e\n",
    "        })\n",
    "        diphoton_final = p4_photons_final[:,0] + p4_photons_final[:,1]\n",
    "        m_yy_final = diphoton_final.mass\n",
    "        pt_yy_final = diphoton_final.pt # Transverse momentum of the diphoton system\n",
    "\n",
    "        # --- Feature Engineering for MVA ---\n",
    "        # Ensure we only use the leading two photons for these features\n",
    "        ph1_pt = events.photon_pt[:,0]\n",
    "        ph2_pt = events.photon_pt[:,1]\n",
    "        ph1_eta = events.photon_eta[:,0]\n",
    "        ph2_eta = events.photon_eta[:,1]\n",
    "        ph1_phi = events.photon_phi[:,0]\n",
    "        ph2_phi = events.photon_phi[:,1]\n",
    "        ph1_e = events.photon_e[:,0] # Not directly used in example features but good to have\n",
    "        ph2_e = events.photon_e[:,1] # Not directly used in example features but good to have\n",
    "\n",
    "        ph1_ptcone20 = events.photon_ptcone20[:,0]\n",
    "        ph2_ptcone20 = events.photon_ptcone20[:,1]\n",
    "        \n",
    "        # Normalized pTs (by m_yy)\n",
    "        ph1_pt_norm = ph1_pt / m_yy_final\n",
    "        ph2_pt_norm = ph2_pt / m_yy_final\n",
    "        \n",
    "        # Angular variables\n",
    "        delta_eta_yy = np.abs(ph1_eta - ph2_eta)\n",
    "        \n",
    "        # Delta phi (handle wrap-around)\n",
    "        delta_phi_yy_raw = ph1_phi - ph2_phi\n",
    "        delta_phi_yy = np.abs(np.mod(delta_phi_yy_raw + np.pi, 2 * np.pi) - np.pi)\n",
    "\n",
    "        delta_R_yy = np.sqrt(delta_eta_yy**2 + delta_phi_yy**2)\n",
    "        \n",
    "        # Normalized isolation\n",
    "        ph1_ptcone20_norm = ph1_ptcone20 / ph1_pt\n",
    "        ph2_ptcone20_norm = ph2_ptcone20 / ph2_pt\n",
    "        \n",
    "        # Collins-Soper frame cosine theta star\n",
    "        # P_yy = ph1_p4 + ph2_p4 (diphoton system 4-momentum)\n",
    "        # In lab frame: p_z_yy = ph1_pz + ph2_pz; E_yy = ph1_E + ph2_E\n",
    "        # Boost to CS frame (where P_yy has only Pz_cs component) is complex.\n",
    "        # A simpler version for H->gg:\n",
    "        cos_theta_cs_yy = np.abs(np.tanh(0.5 * (ph1_eta - ph2_eta))) # Approximation often used\n",
    "\n",
    "        # Event weights\n",
    "        event_weights = events.mcEventWeight if \"mcEventWeight\" in events.fields else ak.ones_like(ph1_pt)\n",
    "        # If mcEventWeight is an array per event, take the first one (nominal)\n",
    "        if hasattr(event_weights, \"ndim\") and event_weights.ndim > 1 and event_weights.type.content.length > 0 :\n",
    "             event_weights = event_weights[:,0]\n",
    "\n",
    "\n",
    "        # Create a dictionary for the DataFrame\n",
    "        df_dict = {\n",
    "            'ph1_pt_norm': ak.to_numpy(ph1_pt_norm),\n",
    "            'ph2_pt_norm': ak.to_numpy(ph2_pt_norm),\n",
    "            'ph1_eta': ak.to_numpy(ph1_eta),\n",
    "            'ph2_eta': ak.to_numpy(ph2_eta),\n",
    "            'delta_eta_yy': ak.to_numpy(delta_eta_yy),\n",
    "            'delta_phi_yy': ak.to_numpy(delta_phi_yy),\n",
    "            'delta_R_yy': ak.to_numpy(delta_R_yy),\n",
    "            'ph1_ptcone20_norm': ak.to_numpy(ph1_ptcone20_norm),\n",
    "            'ph2_ptcone20_norm': ak.to_numpy(ph2_ptcone20_norm),\n",
    "            'cos_theta_cs_yy': ak.to_numpy(cos_theta_cs_yy),\n",
    "            # 'ph1_isTightID': ak.to_numpy(events.photon_isTightID[:,0]).astype(int), # Example if used\n",
    "            # 'ph2_isTightID': ak.to_numpy(events.photon_isTightID[:,1]).astype(int), # Example if used\n",
    "            'm_yy': ak.to_numpy(m_yy_final), # Keep for potential plotting/checks, but NOT an MVA input feature\n",
    "            'pt_yy': ak.to_numpy(pt_yy_final), # Keep for potential plotting/checks\n",
    "            'eventWeight': ak.to_numpy(event_weights),\n",
    "            'isSignal': is_signal_flag\n",
    "        }\n",
    "        df_list.append(pd.DataFrame(df_dict))\n",
    "\n",
    "    if not df_list:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    final_df.dropna(inplace=True) # Clean up any NaNs that might have slipped through\n",
    "    print(f\"Finished processing for pattern. Resulting DataFrame shape: {final_df.shape}\")\n",
    "    return final_df\n",
    "\n",
    "print(\"MC loading and preprocessing function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b795bf2",
   "metadata": {},
   "source": [
    "## 4. Load and Process MC Signal and Background Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "115faa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 17 files for pattern: /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712.*.pool.root.1...\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000016.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000016.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000006.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000006.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000013.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000013.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000010.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000010.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000005.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000005.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000019.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000019.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000009.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000009.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000020.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000020.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000012.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000012.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000007.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000007.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000002.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000002.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000017.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000017.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000014.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000014.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000008.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000008.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000001.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000001.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000018.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000018.pool.root.1:analysis'\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000011.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.38191712._000011.pool.root.1:analysis'\n",
      "Loaded Signal MC. Shape: (0, 0)\n",
      "Processing 1 files for pattern: /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.37110937._000011.pool.root.1...\n",
      "Error opening or reading file /Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.37110937._000011.pool.root.1: [Errno 2] No such file or directory: '/Users/xin/Documents/Documents/College/Phys_392/higgs_lovers/data/DAOD_PHYSLITE.37110937._000011.pool.root.1:analysis'\n",
      "Loaded Background MC. Shape: (0, 0)\n",
      "Error: Both signal and background MC DataFrames are empty. Cannot proceed.\n"
     ]
    }
   ],
   "source": [
    "# Load Signal MC\n",
    "df_signal_mc = load_and_preprocess_mc_for_mva(mc_signal_hyy_file_pattern, 1, tree_name, branches_to_read_mc)\n",
    "print(f\"Loaded Signal MC. Shape: {df_signal_mc.shape}\")\n",
    "if not df_signal_mc.empty:\n",
    "    print(\"Signal MC columns:\", df_signal_mc.columns)\n",
    "    print(\"Signal MC head:\\n\", df_signal_mc.head())\n",
    "\n",
    "# Load Background MC\n",
    "df_background_mc = load_and_preprocess_mc_for_mva(mc_background_hyy_file_pattern, 0, tree_name, branches_to_read_mc)\n",
    "print(f\"Loaded Background MC. Shape: {df_background_mc.shape}\")\n",
    "if not df_background_mc.empty:\n",
    "    print(\"Background MC columns:\", df_background_mc.columns)\n",
    "    print(\"Background MC head:\\n\", df_background_mc.head())\n",
    "\n",
    "# Combine signal and background\n",
    "if not df_signal_mc.empty and not df_background_mc.empty:\n",
    "    df_combined_mc = pd.concat([df_signal_mc, df_background_mc], ignore_index=True)\n",
    "    print(f\"Combined MC DataFrame shape: {df_combined_mc.shape}\")\n",
    "    # Shuffle the DataFrame\n",
    "    df_combined_mc = df_combined_mc.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    print(\"Combined MC DataFrame head after shuffle:\\n\", df_combined_mc.head())\n",
    "elif not df_signal_mc.empty:\n",
    "    print(\"Warning: Background MC is empty. Using only signal MC.\")\n",
    "    df_combined_mc = df_signal_mc.copy()\n",
    "elif not df_background_mc.empty:\n",
    "    print(\"Warning: Signal MC is empty. Using only background MC.\")\n",
    "    df_combined_mc = df_background_mc.copy()\n",
    "else:\n",
    "    print(\"Error: Both signal and background MC DataFrames are empty. Cannot proceed.\")\n",
    "    df_combined_mc = pd.DataFrame() # Ensure it's defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e09cc",
   "metadata": {},
   "source": [
    "### Quick Check of Feature Distributions (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bc7ff1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping feature plots as combined MC DataFrame is empty.\n"
     ]
    }
   ],
   "source": [
    "if not df_combined_mc.empty:\n",
    "    for feature in mva_features_list:\n",
    "        if feature in df_combined_mc.columns:\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist(df_combined_mc[df_combined_mc['isSignal']==1][feature], bins=50, label='Signal', alpha=0.7, density=True, weights=df_combined_mc[df_combined_mc['isSignal']==1]['eventWeight'])\n",
    "            plt.hist(df_combined_mc[df_combined_mc['isSignal']==0][feature], bins=50, label='Background', alpha=0.7, density=True, weights=df_combined_mc[df_combined_mc['isSignal']==0]['eventWeight'])\n",
    "            plt.title(f'Distribution of {feature}')\n",
    "            plt.xlabel(feature)\n",
    "            plt.ylabel('Normalized Events')\n",
    "            plt.legend()\n",
    "            plt.yscale('log')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Warning: Feature '{feature}' not found in DataFrame for plotting.\")\n",
    "else:\n",
    "    print(\"Skipping feature plots as combined MC DataFrame is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb354ce",
   "metadata": {},
   "source": [
    "## 5. Define Features (X) and Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7a40489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot define X and y as combined MC DataFrame is empty.\n"
     ]
    }
   ],
   "source": [
    "if not df_combined_mc.empty:\n",
    "    # Check if all mva_features_list are in df_combined_mc.columns\n",
    "    missing_features = [f for f in mva_features_list if f not in df_combined_mc.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Error: The following MVA features are missing from the DataFrame: {missing_features}\")\n",
    "        print(f\"Available columns: {df_combined_mc.columns.tolist()}\")\n",
    "        # Handle error appropriately, e.g., by raising an exception or exiting\n",
    "        X = pd.DataFrame() # Empty dataframe\n",
    "        y = pd.Series()\n",
    "        weights = pd.Series()\n",
    "    else:\n",
    "        X = df_combined_mc[mva_features_list]\n",
    "        y = df_combined_mc['isSignal']\n",
    "        weights = df_combined_mc['eventWeight'] # Sample weights for training\n",
    "        print(\"Features (X) and Target (y) defined.\")\n",
    "        print(\"X shape:\", X.shape)\n",
    "        print(\"y shape:\", y.shape)\n",
    "        print(\"weights shape:\", weights.shape)\n",
    "else:\n",
    "    print(\"Cannot define X and y as combined MC DataFrame is empty.\")\n",
    "    X = pd.DataFrame()\n",
    "    y = pd.Series()\n",
    "    weights = pd.Series()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1547b0",
   "metadata": {},
   "source": [
    "## 6. Split Data and Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c1d5936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping data splitting and scaling as X is empty.\n"
     ]
    }
   ],
   "source": [
    "if not X.empty:\n",
    "    # Split data into training and testing sets\n",
    "    # Using stratify to ensure similar class proportions in train and test sets\n",
    "    X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "        X, y, weights, test_size=0.3, random_state=42, stratify=y if y.nunique() > 1 else None\n",
    "    )\n",
    "\n",
    "    print(f\"Training set size: {X_train.shape[0]}\")\n",
    "    print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "    # Scale features\n",
    "    # Important: Fit scaler ONLY on training data, then transform both train and test\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Features scaled.\")\n",
    "else:\n",
    "    print(\"Skipping data splitting and scaling as X is empty.\")\n",
    "    X_train_scaled = np.array([])\n",
    "    X_test_scaled = np.array([])\n",
    "    y_train = pd.Series()\n",
    "    y_test = pd.Series()\n",
    "    w_train = pd.Series()\n",
    "    w_test = pd.Series()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48530c",
   "metadata": {},
   "source": [
    "## 7. Train Neural Network\n",
    "Using MLPClassifier from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4569c853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping model training as training data is empty.\n"
     ]
    }
   ],
   "source": [
    "if X_train_scaled.size > 0 :\n",
    "    # Define the MLP model\n",
    "    # These parameters can be tuned (e.g., using GridSearchCV)\n",
    "    mlp_model = MLPClassifier(hidden_layer_sizes=(64, 32), # Example: 2 hidden layers\n",
    "                              activation='relu',\n",
    "                              solver='adam',\n",
    "                              alpha=0.0001, # L2 penalty (regularization)\n",
    "                              batch_size='auto',\n",
    "                              learning_rate='constant',\n",
    "                              learning_rate_init=0.001,\n",
    "                              max_iter=200, # Number of epochs\n",
    "                              shuffle=True,\n",
    "                              random_state=42,\n",
    "                              early_stopping=True, # Stop training if validation score doesn't improve\n",
    "                              n_iter_no_change=10, # Number of iterations with no improvement to wait\n",
    "                              verbose=True)\n",
    "\n",
    "    print(\"Training MLPClassifier model...\")\n",
    "    # Pass sample_weight to the fit method\n",
    "    mlp_model.fit(X_train_scaled, y_train, # sample_weight=w_train # Not directly supported by MLPClassifier.fit for early stopping validation\n",
    "                  # For weighted training with early stopping, one might need a custom loop or use a library\n",
    "                  # that supports it more directly (like Keras/TensorFlow).\n",
    "                  # For now, training unweighted for simplicity of early stopping, or remove early stopping if weights are crucial here.\n",
    "                  # If weights are crucial, consider: mlp_model.fit(X_train_scaled, y_train) and evaluate with weights.\n",
    "                 ) \n",
    "    # Re-fitting without early stopping if weights are essential for the main training loss\n",
    "    # mlp_model = MLPClassifier(...) # re-init without early_stopping\n",
    "    # mlp_model.fit(X_train_scaled, y_train, sample_weight=w_train) # This is how weights would be passed if not using early_stopping's validation set\n",
    "\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    accuracy = mlp_model.score(X_test_scaled, y_test) #, sample_weight=w_test) # score also takes sample_weight\n",
    "    print(f\"Model Accuracy on Test Set: {accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping model training as training data is empty.\")\n",
    "    mlp_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f16c5e",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c74a060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping model evaluation as model or test data is not available.\n"
     ]
    }
   ],
   "source": [
    "if mlp_model and X_test_scaled.size > 0:\n",
    "    # Get predictions (probabilities for the positive class)\n",
    "    y_pred_proba = mlp_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    y_pred_class = mlp_model.predict(X_test_scaled)\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba, sample_weight=w_test)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) - Test Set')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_class, sample_weight=w_test) # Use w_test for weighted CM\n",
    "    print(\"Confusion Matrix (Test Set):\\n\", cm)\n",
    "    \n",
    "    # Plot MVA output score\n",
    "    plt.figure()\n",
    "    plt.hist(y_pred_proba[y_test==1], bins=50, weights=w_test[y_test==1], label='Signal (Test)', alpha=0.7, density=True)\n",
    "    plt.hist(y_pred_proba[y_test==0], bins=50, weights=w_test[y_test==0], label='Background (Test)', alpha=0.7, density=True)\n",
    "    plt.title('MVA Output Score Distribution (Test Set)')\n",
    "    plt.xlabel('MVA Score')\n",
    "    plt.ylabel('Normalized Events')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Skipping model evaluation as model or test data is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0f81dc",
   "metadata": {},
   "source": [
    "## 9. Save Model and Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03cbb809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model or scaler not available. Skipping saving.\n"
     ]
    }
   ],
   "source": [
    "if mlp_model and scaler:\n",
    "    # Save the trained model\n",
    "    joblib.dump(mlp_model, mva_model_path)\n",
    "    print(f\"Trained MVA model saved to: {mva_model_path}\")\n",
    "\n",
    "    # Save the scaler\n",
    "    joblib.dump(scaler, mva_scaler_path)\n",
    "    print(f\"Scaler saved to: {mva_scaler_path}\")\n",
    "else:\n",
    "    print(\"Model or scaler not available. Skipping saving.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f2951a",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "\n",
    "1.  **Update MC File Paths**: Ensure `mc_signal_hyy_file_pattern` and `mc_background_hyy_file_pattern` in Cell 5 point to your actual H→γγ signal and background MC files. You might need to adjust the `branches_to_read_mc` and the feature engineering in `load_and_preprocess_mc_for_mva` (Cell 7) based on your MC content.\n",
    "2.  **Run This Notebook**: Execute all cells to train the MVA and save the `hyy_mva_model.pkl` and `hyy_mva_scaler.pkl` files.\n",
    "3.  **Modify `HyyAnalysisNew.ipynb`**: Proceed to integrate this trained MVA into your main analysis notebook to select events based on the MVA score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
